{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_RNN_from_scratch",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvnLwNJR335e",
        "colab_type": "text"
      },
      "source": [
        "### Building Recurrent Neural Network from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u-EpBGZE4XuM"
      },
      "source": [
        "Data Link : https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvukuh08zcuY",
        "colab_type": "text"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c_zLHZEoMNDI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "515b0934-9d1c-4cee-9b3d-43374cca3771"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q5Sf_ACsvpr",
        "colab_type": "text"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ctai82rr_NJ9",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "import string \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time as time\n",
        "from tqdm import tqdm\n",
        "from numpy.random import randn\n",
        "import glob\n",
        "import operator\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNxYDJ9IP7Pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "362acfb5-2014-4858-efd7-402319df6cfa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmny-gJ2P7P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wPzKb84W1kMZ"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OVan3Yh1UbS",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/Data/NLP/Tweets.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8VXXm53z46I",
        "colab_type": "text"
      },
      "source": [
        "### Check some part of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKIAZCbM7rX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "cf6c36d3-3515-4d92-c882-b9dd57f2f0b6"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4CnEKW2z9u0",
        "colab_type": "text"
      },
      "source": [
        "### Unique sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcdepgwU751M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "851f581c-faa0-49df-c02f-e821b57841a7"
      },
      "source": [
        "df[\"airline_sentiment\"].unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_rinuBR0Cg2",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNI7Wvxa7pM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Dataframe preprocessing\n",
        "map_dic = {'neutral':0,'positive':1,'negative':2}\n",
        "df['label'] = df['airline_sentiment'].map(map_dic)\n",
        "\n",
        "## Train-test split\n",
        "trainingSet, testSet = train_test_split(df, test_size=0.2)\n",
        "#train = pd.Series(trainingSet.label.values,index=trainingSet.text).to_dict()\n",
        "#test = pd.Series(testSet.label.values,index=testSet.text).to_dict()\n",
        "\n",
        "train_text=list(trainingSet.text)\n",
        "train_label= list(trainingSet.label)\n",
        "\n",
        "test_text=list(testSet.text)\n",
        "test_label= list(testSet.label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPLTrAQj6iT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1b7d447-d34a-4b56-f22a-fa26afad0477"
      },
      "source": [
        "df[\"label\"].unique()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rHDL079P7Q3",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing on the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d8Pu-xKP7Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To remove digits\n",
        "def remove_digits(text): \n",
        "    result = re.sub(r'\\d+', '', text) \n",
        "    return result \n",
        "\n",
        "# To remove punctuation \n",
        "def remove_punctuation(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "\n",
        "# To remove whitespace from text \n",
        "def remove_extra_space(text): \n",
        "    return  \" \".join(text.split()) "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHywPZ6YP7Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def part_preprocessing(text):\n",
        "    text = text.lower()\n",
        "    text = remove_digits(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_extra_space(text)\n",
        "    return text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKylOdeYP7RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_text)):\n",
        "    train_text[i] = part_preprocessing(train_text[i])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9NzwipDP7RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_text)):\n",
        "    test_text[i] = part_preprocessing(test_text[i])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGGkOnLRP7RZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a81d2e80-6596-4147-d62d-4b0c4f91acf0"
      },
      "source": [
        "train_text[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'usairways we were moved to a delta direct thank you for the accommodations'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfdB7eAvP7Rg",
        "colab_type": "text"
      },
      "source": [
        "#### Convert Data in required format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l2Ep4wrgP7Rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = {}\n",
        "\n",
        "for a, b in zip(train_text, train_label):\n",
        "    train_data[a]=  b\n",
        "\n",
        "test_data = {}\n",
        "\n",
        "for a, b in zip(test_text, test_label):\n",
        "    test_data[a]=  b\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBzfH4p6P7Rr",
        "colab_type": "text"
      },
      "source": [
        "#### No of Data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rVE8QzxP7Rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ee9118e1-d374-40d8-95f5-4e850f172791"
      },
      "source": [
        "print(\"No of Total Training Data points:\",len(train_data.keys()))\n",
        "print(\"No of Total Test Data points:\",len(test_data.keys()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Total Training Data points: 11512\n",
            "No of Total Test Data points: 2909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lReDj1P7Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer() \n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ZKJqYZP7R5",
        "colab_type": "text"
      },
      "source": [
        "#### Find frequencies and document frequencies of the unique words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG52OtJdP7R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_raw = []\n",
        "count= {}\n",
        "df = {}\n",
        "for text in train_data.keys():\n",
        "    \n",
        "    for word in text.split(\" \"):\n",
        "        word = stemmer.stem(word)\n",
        "        word = lemmatizer.lemmatize(word, pos ='v')\n",
        "        vocab_raw.append(word)\n",
        "        try:\n",
        "            count[word] += 1\n",
        "        except:\n",
        "            count[word] = 1\n",
        "            try:\n",
        "                df[word] = 1\n",
        "            except:\n",
        "                df[word] += 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC6JH0y9P7R_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "485fb689-b5ef-4ff3-8c4a-7c8a5eff578c"
      },
      "source": [
        "sum(count.values())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199022"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR-BTRfP7SP",
        "colab_type": "text"
      },
      "source": [
        "#### TF-IDF values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvx8Gb27P7SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf_vals = []\n",
        "\n",
        "for word in count.keys():\n",
        "    \n",
        "    N = len(train_data)\n",
        "    tf = count[word] / sum(count.values())\n",
        "    idf =  np.log(N/df[word])\n",
        "    tf_idf = tf*idf\n",
        "    tf_idf_vals.append((word,tf_idf))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijCQXgn5P7Si",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "983f9d7c-59d1-45ce-e2e7-b9d6aedcb210"
      },
      "source": [
        "print(\"Total number of words:\",len(vocab_raw))\n",
        "print(\"Total number of unique words:\",len(tf_idf_vals))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words: 199022\n",
            "Total number of unique words: 10589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbLxMeidP7Ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vocab_list = list(zip(words, counts))\n",
        "vocab_list = tf_idf_vals\n",
        "vocab_list.sort(key=operator.itemgetter(1),reverse =True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utzGAjeQP7Sy",
        "colab_type": "text"
      },
      "source": [
        "#### Filterd Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpdTLbalP7Sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = [vocab_list[i][0] for i in range(len(vocab_list))]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9prMY21P7S5",
        "colab_type": "text"
      },
      "source": [
        "#### Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_hJ583SP7S6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(tokens_list):\n",
        "    all_stopwords = stopwords.words('english')\n",
        "    all_stopwords.append('')\n",
        "    all_stopwords.append('br')\n",
        "    \n",
        "    out = [word for word in tokens_list if not word in all_stopwords]\n",
        "    return out"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weDBGeH7P7TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_vocab = remove_stopwords(vocab)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcfjevnoP7TJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a89ed0b-1091-40c1-bd15-890a016b63b9"
      },
      "source": [
        "vocab_size = len(filtered_vocab)\n",
        "print(\"Length of the vocabulary:\",vocab_size)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the vocabulary: 10488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDSQUDlQP7TQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ba2b62f-bdbe-4d50-eb2c-19cf772237fa"
      },
      "source": [
        "filtered_vocab = filtered_vocab[:4999] ##5000 words considered\n",
        "filtered_vocab.append(\"OTHERS\")\n",
        "vocab_size = len(filtered_vocab)\n",
        "print(\"Length of the working vocabulary:\",vocab_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the working vocabulary: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSgivvIXuLe-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7741eb4-f7bf-4c1e-e329-805fb45ca4b5"
      },
      "source": [
        "# Assign indices to each word.\n",
        "word_to_idx = { w: i for i, w in enumerate(filtered_vocab) }\n",
        "idx_to_word = { i: w for i, w in enumerate(filtered_vocab) }\n",
        "#print(word_to_idx['thank']) # 16 (this may change)\n",
        "print(\"Word corresponding to 4 th index:\",idx_to_word[4]) "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word corresponding to 4 th index: southwestair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPNfGIjyP7Td",
        "colab_type": "text"
      },
      "source": [
        "### Create Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BXJWu-JKuLuw",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def encoded_inputs(text):\n",
        "  \n",
        "  inputs = []\n",
        "  ## Preprocess\n",
        "  text = part_preprocessing(text)\n",
        "  words = text.split(' ')\n",
        "  words = remove_stopwords(words)\n",
        "\n",
        "  ## Encoding\n",
        "  for word in words:\n",
        "    word = stemmer.stem(word)\n",
        "    word = lemmatizer.lemmatize(word, pos ='v')\n",
        "    encoded_word = np.zeros((vocab_size, 1))\n",
        "    if word not in word_to_idx.keys():\n",
        "      encoded_word[word_to_idx[\"OTHERS\"]] = 1\n",
        "    else:  \n",
        "      encoded_word[word_to_idx[word]] = 1\n",
        "    inputs.append(encoded_word)\n",
        "\n",
        "  return inputs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXeVZDm3P7Tm",
        "colab_type": "text"
      },
      "source": [
        "### RNN class( Whole Algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cmj9ATFEuL7o",
        "colab": {}
      },
      "source": [
        "class Recurrent_NN:\n",
        "\n",
        "  def __init__(self, input_size,  hidden_size ,output_size, learning_rate= 0.001):\n",
        "        \n",
        "    # Initialize Weights and biases\n",
        "\n",
        "    ## Weights corresponding to input and hidden layer\n",
        "    self.Wxh = randn(hidden_size, input_size) / 1000\n",
        "\n",
        "    ## Weights corresponding to two hidden layers\n",
        "    self.Whh = randn(hidden_size, hidden_size) / 1000\n",
        "\n",
        "    ## Weights corresponding to ouput and hidden layer\n",
        "    self.Why = randn(output_size, hidden_size) / 1000\n",
        "\n",
        "    ## Biases corresponding to hidden layer\n",
        "    self.bh = np.zeros((hidden_size, 1))\n",
        "\n",
        "    ## Biases corresponding to output layer\n",
        "    self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    ## Learning rate\n",
        "    self.learning_rate = learning_rate\n",
        "  \n",
        "  ## Activation Functions\n",
        "\n",
        "  ## Tanh activation\n",
        "\n",
        "  def tanh(self,x):\n",
        "    return np.tanh(x)\n",
        "  \n",
        "  ## Softmax Activation\n",
        "\n",
        "  def softmax(self,x):\n",
        "    return np.exp(x) / sum(np.exp(x))\n",
        "\n",
        "  ## Update parameter using gradient descent algorithm\n",
        "\n",
        "  def update_param(self, param_grad_pair):\n",
        "     x =param_grad_pair[0]\n",
        "     d_x = param_grad_pair[1]\n",
        "     x -= self.learning_rate * d_x\n",
        "     return x\n",
        "\n",
        "  ## To overcome exploding gradient problem in backprop\n",
        "\n",
        "  def not_explode_grad(self,x):\n",
        "     x = np.clip(x, -1, 1)\n",
        "     return x\n",
        "    \n",
        "  ## Compute loss in the forward prop\n",
        "\n",
        "  def calculate_loss(self,probs,target):\n",
        "     loss = - np.log(probs[target])\n",
        "     return loss\n",
        "\n",
        "  ## Forward Propagation\n",
        "\n",
        "  def forward_prop(self, inputs,target):\n",
        "\n",
        "    ## Inputs and targets\n",
        "    self.inputs = inputs\n",
        "    self.target = int(target)\n",
        "\n",
        "    ## Store h values in different time steps (Memory of RNN)\n",
        "    self.h_values = {}\n",
        "\n",
        "    ## Initialize the hidden node values \n",
        "    h = np.zeros((self.Whh.shape[0], 1))\n",
        "    self.h_values[0] = h\n",
        "    \n",
        "    for i, x in enumerate(inputs):\n",
        "      \n",
        "      ## Previous hidden layer values is being used here\n",
        "      Z = self.Wxh @ x + self.Whh @ h + self.bh\n",
        "\n",
        "      ## Tanh activation on hidden layer\n",
        "      h = self.tanh(Z)\n",
        "\n",
        "      ## Store the current h for next time step\n",
        "      self.h_values[i + 1] = h\n",
        "\n",
        "    ## Compute output in the final time step\n",
        "    y = self.Why @ h + self.by\n",
        "    \n",
        "    ## Softmax for probabilities\n",
        "    probs = self.softmax(y)\n",
        "    self.probs = probs\n",
        "\n",
        "    ## Calculate Loss\n",
        "    loss = self.calculate_loss(probs,target)\n",
        "    \n",
        "    return(y, h, probs, loss)\n",
        "\n",
        "  \n",
        "  ## Backpropagation    \n",
        "\n",
        "  def BPTT(self):\n",
        "    \n",
        "    ## Gradient of loss w.r.t y\n",
        "    d_y = self.probs\n",
        "    d_y[self.target] -= 1\n",
        "    \n",
        "    # Initialize the gradients of loss w.r.t the paramters\n",
        "    d_Whh = np.zeros(self.Whh.shape)\n",
        "    d_Wxh = np.zeros(self.Wxh.shape)\n",
        "    d_bh = np.zeros(self.bh.shape)\n",
        "\n",
        "    ## No of inputs for a input data\n",
        "    N = len(self.inputs)\n",
        "\n",
        "\n",
        "    # Following gradient depends only on ouput and last time step hidden values\n",
        "    d_Why = d_y @ self.h_values[N].T\n",
        "    d_by = d_y\n",
        "\n",
        "    # Gradient of loss w.r.t last time step h values\n",
        "    d_h = self.Why.T @ d_y\n",
        "\n",
        "    ## Backpropagate through time.\n",
        "    for t in reversed(range(N-1,-1,-1)):\n",
        "\n",
        "      # Derivative of tanh(x) w.r.t x is (1- tanh(x)^2)\n",
        "      ## Need the following value in computation of gradients\n",
        "      temp = ((1 - self.h_values[t + 1] ** 2) * d_h)\n",
        "\n",
        "      # Gradient of loss w.r.t bh\n",
        "      d_bh += temp\n",
        "\n",
        "      # Gradient of loss w.r.t Whh\n",
        "      d_Whh += temp @ self.h_values[t].T\n",
        "\n",
        "      # Gradient of loss w.r.t Wxh\n",
        "      d_Wxh += temp @ self.inputs[t].T\n",
        "\n",
        "      # Gradient of loss w.r.t h\n",
        "      d_h = self.Whh @ temp\n",
        "\n",
        "    ## Get rid of exploding gradients.\n",
        "  \n",
        "    d_Wxh, d_Whh, d_Why, d_bh, d_by = list(map(self.not_explode_grad,[d_Wxh, d_Whh, d_Why, d_bh, d_by]))\n",
        "\n",
        "    ## Update weights and biases using gradient descent.\n",
        "\n",
        "    param_grad_pair = [(self.Whh,d_Whh),(self.Wxh,d_Wxh),(self.Why,d_Why),(self.bh,d_bh),(self.by,d_by)]\n",
        "    \n",
        "    self.Whh,self.Wxh,self.Why,self.bh,self.by = list(map(self.update_param,param_grad_pair))\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlsIpzouP7UC",
        "colab_type": "text"
      },
      "source": [
        "### Function for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iucVp8FHHFck",
        "colab": {}
      },
      "source": [
        "def compute_loss_accuracy(data, BPTT=True):\n",
        "\n",
        "    total_cost = 0\n",
        "    correct_pred= 0\n",
        "\n",
        "    items = list(data.items())\n",
        "    ## No of total data points\n",
        "    N = len(items)\n",
        "    ## Shuffle the data\n",
        "    random.shuffle(items)\n",
        "\n",
        "    \n",
        "\n",
        "    for x, y in items:\n",
        "\n",
        "        inputs = encoded_inputs(x)\n",
        "        true_label = int(y)\n",
        "\n",
        "        out, h, probs, loss = model.forward_prop(inputs,true_label)\n",
        "        total_cost += loss\n",
        "        correct_pred += int(np.argmax(probs) == true_label)\n",
        "\n",
        "        ## For test data we don't do backpropagation\n",
        "        if BPTT:\n",
        "          model.BPTT()\n",
        "    \n",
        "    avg_loss = total_cost / N\n",
        "    accuracy = correct_pred / N\n",
        "\n",
        "    return(avg_loss, accuracy)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DP40A-aTTB99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b6cb250e-b2c0-497b-e2f1-c7a89e402b55"
      },
      "source": [
        "print(\"Length of Training Data:\",len(train_data.keys()))\n",
        "print(\"Length of Test Data:\",len(test_data.keys()))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Training Data: 11512\n",
            "Length of Test Data: 2909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mV3_7g9P7UZ",
        "colab_type": "text"
      },
      "source": [
        "### Training and Testing loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9Ji0qZLHPXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d8f7861e-b7fc-447b-865a-dae714bbbb25"
      },
      "source": [
        "input_size = vocab_size\n",
        "hidden_size = 50\n",
        "output_size = 3\n",
        "learning_rate = 0.01\n",
        "model = Recurrent_NN(input_size,hidden_size, output_size, learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    training_loss, training_accuracy = compute_loss_accuracy(train_data)  \n",
        "    print('Epoch {}'.format(epoch + 1)) \n",
        "    print('Training Loss: {} and Training Accuracy: {}'.format(training_loss, training_accuracy))\n",
        "\n",
        "    test_loss, test_accuracy = compute_loss_accuracy(test_data, BPTT=False)\n",
        "    print('Test Loss: {} and Test Accuracy: {}'.format(test_loss, test_accuracy))\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print(\"Time taken to complete epoch {}: {}\".format(epoch + 1, end - start))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Training Loss: [0.91398851] and Training Accuracy: 0.629864489228631\n",
            "Test Loss: [0.8995403] and Test Accuracy: 0.640770024063252\n",
            "Time taken to complete epoch 1: 232.96435952186584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHlcORz32doC",
        "colab_type": "text"
      },
      "source": [
        "    This is a very simple RNN model which is designed from scratch. It can be generalized. It helped me to understand how RNN can be modelled. Even if one doesn't understand the whole algorithm,they can simply use Keras/Tensorflow. Finest of libraries are available.But this work is basically to understand the whole algorithm. Check the documentation for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BSZmXpuqGlLA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}