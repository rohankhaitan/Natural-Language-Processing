{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "First_part(Write_corpus_&_preprocessing)",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ9yN1lNgjX6",
        "colab_type": "text"
      },
      "source": [
        "    Done on Jupyter notebook separately.\n",
        "    It took lot of time to read and preprocess the data in my low specification system.\n",
        "    The functions are applied on small chunks of data and the results were combined to get the final output.\n",
        "    After creating the final corpus, I shifted to Colab.\n",
        "    To read and preprocess the data in my low specification was the toughest job. Most of the time the kernel became dead or I got memoryerror.\n",
        "    That is why I decided to do it in chunks and then combine the result.\n",
        "    It can be done on colab but as I spent most of my time writing and running this code I decided to share this file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB6osSwidNMr",
        "colab_type": "text"
      },
      "source": [
        "### Write the Corpus in a text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VvrUPxtFdNMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wiki_dump_reader import Cleaner, iterate\n",
        "\n",
        "def write_corpus():\n",
        "    corpus_file = '/home/rohan/CMI/SEM_3/Text_analytics/Bengali_corpus.txt'\n",
        "    page_count = 0\n",
        "    cleaner = Cleaner()\n",
        "    with open(corpus_file, 'w', encoding='utf-8') as output:\n",
        "        for title, text in iterate('/home/rohan/CMI/SEM_3/Text_analytics/bnwiki-latest-pages-articles.xml'):\n",
        "            text = cleaner.clean_text(text)\n",
        "            cleaned_text, links = cleaner.build_links(text)\n",
        "            output.write(title + '\\n' + cleaned_text + '\\n')\n",
        "            page_count += 1\n",
        "            if page_count % 1000 == 0:\n",
        "                print('Pages dumped = ', page_count)\n",
        "\n",
        "    output.close()\n",
        "write_corpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4CIglQ0dNM0",
        "colab_type": "text"
      },
      "source": [
        "### Read the Corpus from the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWq1KQ_NdNM2",
        "colab_type": "code",
        "outputId": "08ccc189-0345-4d88-dd5e-e77f05a7d567",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start=time.time()\n",
        "file = open('/home/rohan/CMI/SEM_3/Text_analytics/Bengali_corpus.txt','r')\n",
        "data = file.read()\n",
        "end=time.time()\n",
        "print(end-start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.090218544006348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYoA8Ec_dNM9",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDCalTwAdNM_",
        "colab_type": "text"
      },
      "source": [
        "#### Function to remove the special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL4PKkzndNNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "special_characters=\"=-/,!@|#$&:;$\"\n",
        "def remove_special_characters(text):\n",
        "    for i in range(len(special_characters)):\n",
        "        text =  re.sub(str(special_characters[i]),\"\", text)\n",
        "    text =  re.sub(\"\\n\",\" \", text)\n",
        "    return(text)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wysLzkVgdNNG",
        "colab_type": "text"
      },
      "source": [
        "#### A function to remove any non-bengali words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REuoEaxadNNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_bengali_word(word): \n",
        "    try:        \n",
        "        lang = unicodedata.name(word.strip()[0])        \n",
        "        if 'BENGALI' in lang and 'DIGIT' not in lang:            \n",
        "            return True        \n",
        "        else:            \n",
        "            return False    \n",
        "    except:        \n",
        "        return False "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWVHkymhdNNN",
        "colab_type": "code",
        "outputId": "1f8d5827-469c-491d-e7b9-9da8d2b0e8d4",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "244751468"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-hQUDYgdNNT",
        "colab_type": "text"
      },
      "source": [
        "##### As I have very low specification system, operations on the whole data lead to memoryerror or dead kernel. So to avoid that and to be able to work in my own system, I decided to apply the functions on small chunks of data and combining the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifog924NdNNV",
        "colab_type": "text"
      },
      "source": [
        "#### Removing the special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xfiU7kFdNNX",
        "colab_type": "code",
        "outputId": "aea5dfaf-caef-417e-e08d-740684973e39",
        "colab": {}
      },
      "source": [
        "##It is done by reading the data in chunks instead of reading the whole data\n",
        "\n",
        "start=time.time()\n",
        "window_size=100000\n",
        "mod_data=\"\"\n",
        "i=0\n",
        "while i<len(data):\n",
        "        mod_data+=remove_special_characters(data[i:i+100000])\n",
        "        i=i+100000\n",
        "        if i%50000000==0:\n",
        "            print(\"Proceeding, Time:\",time.time())  ## To check the procedding\n",
        "print(i)\n",
        "## For the last chunk of data\n",
        "mod_data+=remove_special_characters(data[i-100000:])\n",
        "end=time.time()\n",
        "print(\"Time taken:\",end-start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proceeding, Time: 1573368913.857451\n",
            "Proceeding, Time: 1573368939.6548204\n",
            "Proceeding, Time: 1573368964.2493842\n",
            "Proceeding, Time: 1573368989.1016688\n",
            "244800000\n",
            "Time taken: 122.74754285812378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhDZy_xJdNNf",
        "colab_type": "code",
        "outputId": "86fe4364-7fc3-4b34-bb27-f7bfe2ecb8ee",
        "colab": {}
      },
      "source": [
        "len(mod_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "239849139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqID3vDodNNl",
        "colab_type": "text"
      },
      "source": [
        "#### A function to find space from a text nearest to any given index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE0-rkDNdNNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_nearst_index_for_space(text,index):\n",
        "    window=25\n",
        "    for i in range(window):\n",
        "        if text[index-i]==\" \":\n",
        "            return index-i\n",
        "        elif text[index+i]==\" \":\n",
        "            return index+i\n",
        "        else:\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgETzUwkdNNv",
        "colab_type": "text"
      },
      "source": [
        "#### Removing the non-bengali words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXYx8KEjdNNx",
        "colab_type": "text"
      },
      "source": [
        "#### Target is to apply the function on small chunks of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1QTJQxwdNNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modified_corpus=\"\"\n",
        "i=k=0\n",
        "while i<len(data):\n",
        "    index=find_nearst_index_for_space(mod_data,i+100000)\n",
        "    for word in mod_data[i:index].split():\n",
        "        if len(word)>2 and check_bengali_word(word):\n",
        "            modified_corpus+=word+\"\"  \n",
        "    i=index\n",
        "##now for the last chunk of data    \n",
        "for word in mod_data[i:].split():\n",
        "    if len(word)>2 and check_bengali_word(word):\n",
        "        modified_corpus+=word+\"\"     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0I2a0OHdNN4",
        "colab_type": "text"
      },
      "source": [
        "#### Saving the modified corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcK1r6XLdNN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textfile = open('/home/rohan/CMI/SEM_3/Text_analytics/Corpus.txt', 'w')\n",
        "textfile.write(modified_corpus)\n",
        "textfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwWQmSFKdNN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}