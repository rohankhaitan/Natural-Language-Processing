{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibberish Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This was a hackathon Competition. Actual labels of test data was not provided for obvious reasons. The test accuracy was checked in their platform. So here you can check only the training accuarcy.(No validation set)\n",
    "    \n",
    "    Learning the context based on the characters (using the CNN model) turned out to be the suitable approach here(worked better than using n-gram)\n",
    "    \n",
    "    Takes time. Used GPU to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = r\"C:\\Users\\....\\trainData.csv\"\n",
    "\n",
    "df = pd.read_csv(data_source, header=None)\n",
    "df = df.iloc[1:]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sk vienuh tw lep mamqvuh mvlekrp m qvqvmvqjuht...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>g zmva mqvbhtwi wenamdfu hezletwgzqvuhtwamuluh...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>lrvimv u cypmvu luhtwa mguuhraqv twmkiwtwq vez...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>xepmuhul m vezezkrqvuhtwamuluhtwmkpmtvenuhtwyp...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>qgqv lrvimvleulqvuhtwamuluhsatv uhlrvikramuhul...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   1\n",
       "0  sk vienuh tw lep mamqvuh mvlekrp m qvqvmvqjuht...   5\n",
       "1  g zmva mqvbhtwi wenamdfu hezletwgzqvuhtwamuluh...   7\n",
       "2  lrvimv u cypmvu luhtwa mguuhraqv twmkiwtwq vez...  11\n",
       "3  xepmuhul m vezezkrqvuhtwamuluhtwmkpmtvenuhtwyp...   6\n",
       "4  qgqv lrvimvleulqvuhtwamuluhsatv uhlrvikramuhul...   3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '7', '11', '6', '3', '10', '1', '8', '9', '4', '2', '0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df[0].values\n",
    "\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=500, padding='post')\n",
    "\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "\n",
    "train_classes = df[1].values\n",
    "train_class_list = [x for x in train_classes]\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_classes = to_categorical(train_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data\n",
    "len(train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 69)           4830      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 256)          123904    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 494, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 164, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 162, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 162, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 160, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 160, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 158, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 158, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 52, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 13312)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              13632512  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                12300     \n",
      "=================================================================\n",
      "Total params: 15,413,738\n",
      "Trainable params: 15,413,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 500\n",
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 69\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 12\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "# Embedding weights\n",
    "embedding_weights = []  \n",
    "embedding_weights.append(np.zeros(vocab_size))  \n",
    "\n",
    "for char, i in tk.word_index.items(): \n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i - 1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print('Load')\n",
    "\n",
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size + 1,\n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "\n",
    "# Model Construction\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  \n",
    "\n",
    "# Embedding\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x) \n",
    "x = Flatten()(x) \n",
    "\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  \n",
    "    x = Dropout(dropout_p)(x)\n",
    "\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  \n",
    "model.summary()\n",
    "\n",
    "# Shuffle\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_data[indices]\n",
    "y_train = train_classes[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      " - 16s - loss: 2.3395 - acc: 0.1568\n",
      "Epoch 2/50\n",
      " - 12s - loss: 2.0774 - acc: 0.2516\n",
      "Epoch 3/50\n",
      " - 12s - loss: 1.7736 - acc: 0.3760\n",
      "Epoch 4/50\n",
      " - 12s - loss: 1.4074 - acc: 0.5012\n",
      "Epoch 5/50\n",
      " - 12s - loss: 1.1644 - acc: 0.5865\n",
      "Epoch 6/50\n",
      " - 12s - loss: 1.0211 - acc: 0.6386\n",
      "Epoch 7/50\n",
      " - 12s - loss: 0.8794 - acc: 0.6905\n",
      "Epoch 8/50\n",
      " - 12s - loss: 0.7496 - acc: 0.7415\n",
      "Epoch 9/50\n",
      " - 12s - loss: 0.6213 - acc: 0.7838\n",
      "Epoch 10/50\n",
      " - 12s - loss: 0.5133 - acc: 0.8237\n",
      "Epoch 11/50\n",
      " - 12s - loss: 0.3935 - acc: 0.8678\n",
      "Epoch 12/50\n",
      " - 12s - loss: 0.3146 - acc: 0.8942\n",
      "Epoch 13/50\n",
      " - 12s - loss: 0.2488 - acc: 0.9167\n",
      "Epoch 14/50\n",
      " - 12s - loss: 0.1786 - acc: 0.9409\n",
      "Epoch 15/50\n",
      " - 12s - loss: 0.1530 - acc: 0.9484\n",
      "Epoch 16/50\n",
      " - 12s - loss: 0.1262 - acc: 0.9596\n",
      "Epoch 17/50\n",
      " - 12s - loss: 0.1166 - acc: 0.9641\n",
      "Epoch 18/50\n",
      " - 12s - loss: 0.1114 - acc: 0.9639\n",
      "Epoch 19/50\n",
      " - 12s - loss: 0.1214 - acc: 0.9614\n",
      "Epoch 20/50\n",
      " - 12s - loss: 0.0836 - acc: 0.9746\n",
      "Epoch 21/50\n",
      " - 12s - loss: 0.0796 - acc: 0.9748\n",
      "Epoch 22/50\n",
      " - 12s - loss: 0.0655 - acc: 0.9791\n",
      "Epoch 23/50\n",
      " - 12s - loss: 0.0612 - acc: 0.9803\n",
      "Epoch 24/50\n",
      " - 12s - loss: 0.0631 - acc: 0.9800\n",
      "Epoch 25/50\n",
      " - 12s - loss: 0.0681 - acc: 0.9792\n",
      "Epoch 26/50\n",
      " - 12s - loss: 0.0593 - acc: 0.9825\n",
      "Epoch 27/50\n",
      " - 12s - loss: 0.0654 - acc: 0.9794\n",
      "Epoch 28/50\n",
      " - 12s - loss: 0.0641 - acc: 0.9799\n",
      "Epoch 29/50\n",
      " - 12s - loss: 0.0643 - acc: 0.9803\n",
      "Epoch 30/50\n",
      " - 12s - loss: 0.0494 - acc: 0.9849\n",
      "Epoch 31/50\n",
      " - 13s - loss: 0.0448 - acc: 0.9859\n",
      "Epoch 32/50\n",
      " - 13s - loss: 0.0436 - acc: 0.9865\n",
      "Epoch 33/50\n",
      " - 12s - loss: 0.0534 - acc: 0.9828\n",
      "Epoch 34/50\n",
      " - 12s - loss: 0.0437 - acc: 0.9861\n",
      "Epoch 35/50\n",
      " - 12s - loss: 0.0443 - acc: 0.9866\n",
      "Epoch 36/50\n",
      " - 12s - loss: 0.0620 - acc: 0.9816\n",
      "Epoch 37/50\n",
      " - 12s - loss: 0.0471 - acc: 0.9847\n",
      "Epoch 38/50\n",
      " - 12s - loss: 0.0430 - acc: 0.9871\n",
      "Epoch 39/50\n",
      " - 12s - loss: 0.0501 - acc: 0.9839\n",
      "Epoch 40/50\n",
      " - 12s - loss: 0.0476 - acc: 0.9847\n",
      "Epoch 41/50\n",
      " - 12s - loss: 0.0346 - acc: 0.9896\n",
      "Epoch 42/50\n",
      " - 12s - loss: 0.0404 - acc: 0.9878\n",
      "Epoch 43/50\n",
      " - 13s - loss: 0.0385 - acc: 0.9880\n",
      "Epoch 44/50\n",
      " - 12s - loss: 0.0397 - acc: 0.9885\n",
      "Epoch 45/50\n",
      " - 13s - loss: 0.0469 - acc: 0.9849\n",
      "Epoch 46/50\n",
      " - 12s - loss: 0.0605 - acc: 0.9817\n",
      "Epoch 47/50\n",
      " - 12s - loss: 0.0330 - acc: 0.9893\n",
      "Epoch 48/50\n",
      " - 12s - loss: 0.0456 - acc: 0.9857\n",
      "Epoch 49/50\n",
      " - 12s - loss: 0.0386 - acc: 0.9886\n",
      "Epoch 50/50\n",
      " - 13s - loss: 0.0477 - acc: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a7aa272b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=50,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=r\"C:\\Users\\....\\testdata\\testData.csv\"\n",
    "\n",
    "test_df = pd.read_csv(source, header=None)\n",
    "test_df = test_df.iloc[1:]\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = tk.texts_to_sequences(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pad_sequences(test_texts, maxlen=500, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test_data, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\....\\rohank_output.txt\", \"w\") \n",
    "for i in range(len(out)):\n",
    "        z=np.argmax(out[i])\n",
    "        f.write(\"%d\" % z)\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
